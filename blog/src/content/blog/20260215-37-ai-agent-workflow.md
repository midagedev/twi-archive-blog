---
title: "pod에 간단한 이미지 띄우는 것 까진 무리 없었는데, 서버에서 클로드코드 돌리는거 너무 빡시다."
description: "서버에서 에이전트를 실제 작업에 붙일 때 겪은 병목과, 검증 루프·컨텍스트 예산·자가 수정 경로를 정리한 기록."
pubDate: 2025-07-03T10:50:15+00:00
source: twitter
tags:
  - ai-agent
  - llm-workflow
  - context-budget
  - feedback-loop
  - gitops
---

pod에 이미지를 띄우는 단계까지는 무리 없었다. 문제는 서버에서 클로드코드를 계속 돌리며 수정-확인을 반복할 때였다. 데모 1회 성공과 운영 흐름은 난도가 달랐다. 터미널에서 바로 에이전트를 켜면 편한데, 리소스가 부족하면 작업 자체가 끊겼다. 결국 4GB로 올린 뒤에야 서버에서 gitops용 레포를 만들고 GitHub에 반영하고, 생성한 YAML을 ArgoCD에 넣는 단계가 한 흐름으로 이어졌다.

그래도 아직 동료처럼 맡기지는 않는다. 체감은 짧은 피드백에는 유용하지만, 맥락이 길어지면 쉽게 흔들리는 신입에 가깝다. 그래서 경계를 먼저 정했다. 모호한 판단은 사람이 기준을 쥐고, 에이전트는 좁은 단위 실행과 재시도에 집중시킨다. 이 분리가 없으면 실패 원인이 사람 판단인지 모델 출력인지 섞여서 수정 속도가 급격히 떨어졌다.

판단이 애매할 때는 클로드 결과를 코덱스와 제미나이에 보내 피드백을 받고, 그걸 다시 클로드에 넣는 수동 루프를 몇 번 돌렸다. 일부는 개선됐지만, 잘못된 방향으로 같이 동조하는 경우도 분명히 있었다. 모델을 늘린다고 자동으로 객관성이 생기진 않았다. 답을 많이 모으는 것보다, 어떤 조건을 통과해야 정답으로 인정할지 검증 기준을 먼저 고정하는 쪽이 효과적이었다.

실무에서 더 큰 차이는 컨텍스트 예산과 피드백 경로에서 났다. /context를 보면 시작 시점부터 클로드코드 자체 컨텍스트가 꽤 큰 비중을 차지했다. 결과 확인 절차를 길게 두면 본 작업 전에 토큰을 먼저 써버린다. 그래서 확인 방식을 한 번 더 감싸, 에이전트가 에러를 직접 보고 고치고 다시 확인하게 만들었다. 잘 돌아간 순간에는 다음에도 유지할 규칙을 물어 agents_md에 남겼다. 정량 지표를 길게 쌓은 단계는 아니지만, 효율은 모델 성능보다 에러를 사람이 중계하지 않는 루프를 얼마나 빨리 만드는지에 가까웠다.
