---
title: AI 코딩 도입에서 품질 기준을 먼저 합의해야 했던 이유
description: 개인 편향, 리뷰 압력 저하, 사용 지표 노출을 겪으면서 AI 코딩을 팀에 붙일 때 필요한 운영 기준을 정리했다.
pubDate: 2023-04-14T03:56:47+00:00
source: twitter
tags:
  - ai
  - engineering
  - code-quality
  - team-management
---

AI 도구를 쓰기 시작하고 가장 먼저 드러난 문제는 기술보다 태도였다. 내 커밋에서 Copilot이나 ChatGPT를 쓰는 건 합리화하면서, 다른 사람 커밋을 보면 “생각한 흔적이 부족하다”는 판단을 더 쉽게 내렸다. 같은 결과물을 두고도 사람에 따라 기준을 다르게 적용하는 순간, 리뷰는 품질 점검이 아니라 취향 평가가 된다. 도구 도입 초반에 팀 갈등이 커지는 이유도 여기에서 시작된다고 봤다.

그다음 단계에서는 반대로 기준이 너무 느슨해졌다. 팀에서 코드 품질을 집요하게 묻는 사람이 줄어들자, Claude 제안을 거의 바이패스로 넣는 일이 늘었다. 예전에는 팀 코드의 문체와 경계에 맞추는 작업을 했는데, 어느 순간 “편하니까 일단 넣고 보자”로 기울었다. 속도는 올라가도 유지보수 비용은 뒤로 미뤄진다. 그래서 프롬프트를 잘 쓰는 법보다, 머지 전 최소 품질 기준을 다시 문장으로 박아두는 쪽이 더 효과적이었다.

운영 관점에서 더 크게 느낀 건 관측 가능성의 비대칭이다. 팀 비즈니스 플랜에서는 요청량뿐 아니라 제안 수용/거절 같은 상세 지표가 매니저에게 보일 수 있다. 의미 있는 신호일 수는 있지만, 사용자는 어디까지 노출되는지 체감하기 어렵다. 이 상태에서 숫자만으로 개인 기여를 해석하면 왜곡이 생긴다. 지표는 참고값으로 두고, 어떤 데이터가 누구에게 보이는지부터 팀에 명확히 공유해야 한다.

결국 “도착지가 맞는 코드인지”는 프롬프트 스킬만으로 학습되지 않았다. 좋은 동료와 리뷰 맥락 안에서, 이 요구사항을 위해 만든 부채가 싼지 비싼지, 복잡도를 올린 만큼 이득이 있는지, 더 단순한 대안은 없는지를 계속 따져야 했다. 기획서 기반 프로토타이핑과 질문 목록 생성처럼 LLM이 강한 구간은 분명히 있다. 다만 그 결과를 최종 결정으로 쓰기보다, 판단을 빠르게 시작하게 해주는 초안으로 다루는 편이 안전했다.

최근에는 팀 전원이 에이전트를 쓰는 상태까지는 만들었고 9인 구독도 승인됐다. 하지만 이걸 성과로 단정하지는 않는다. 사용량이 늘어도 의사결정 품질이 같이 오르는지는 별도 검증이 필요하다. 내가 정리한 결론은 단순하다. AI 코딩 도입의 핵심은 도구 선택이 아니라, 사람마다 달라지기 쉬운 기준을 팀의 공통 기준으로 바꾸는 운영 설계다.
