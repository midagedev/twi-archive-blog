---
title: 팀 워크플로우에서 LLM 코딩 가드레일을 세운 과정
description: 핸즈온 성공과 실패, 리뷰 의존의 회귀를 거치며 팀 개발에서 LLM 보조 코딩을 운영한 방식과 검증 원칙을 정리했다.
pubDate: 2025-04-23T10:59:45+00:00
source: twitter
tags:
  - llm
  - 팀개발
  - 코드리뷰
  - 테스트
  - 개발프로세스
---

팀에서 LLM 보조 코딩을 실무에 붙여보니, 모델 성능보다 먼저 작업 단위와 검증 경계가 문제였다. 클로드코드 핸즈온을 같은 방식으로 두 번 진행했을 때 프론트엔드 쪽은 잘 굴러갔지만, 백엔드는 API 통합 테스트를 처음 넣는 구간에서 환경 설정이 막히며 토큰만 크게 소모됐다. 이 경험 이후 “모델이 똑똑한가”보다 “재현 가능한 실행 환경이 먼저 준비됐는가”를 시작 조건으로 보게 됐다.

프롬프트를 반복해도 의도가 전달되지 않으면 계속 밀지 않고, 보통 다섯 번 안에서 `/clear`로 컨텍스트를 끊었다. 길게 끌수록 이전 방향성에 끌려가며 비용만 늘어나는 패턴이 반복됐기 때문이다. PR 메시지에 제미나이 리뷰를 붙인 것도 처음 목적은 리뷰어 부담을 줄여 승인 속도를 높이려는 것이었다. 그런데 실제로는 PR 올리기 전에 스스로 한 번 더 확인하는 절차가 생기면서 단순 실수가 먼저 줄었다.

문제는 그다음 단계에서 나왔다. 리뷰 결과를 과신하고 가볍게 머지한 변경들에서 이슈가 몇 번 발생했다. “논리는 어느 정도 검증됐겠지”라는 기대가 틀린 경우였다. 더 극단적으로는 테스트 실패를 해결한다며 테스트 케이스를 지워 통과시키거나, 임포트가 안 되자 테스트 대상을 케이스에 맞게 바꿔 모킹하는 출력도 나왔다. 테스트 통과라는 신호만 따라가면 품질은 쉽게 무너진다는 걸 다시 확인했다.

그래서 지금은 한 모델에 토큰을 더 태우기보다 교차 검증 가드레일을 둔다. 세션을 비워도 비슷한 결론 편향이 남을 때가 있어 다른 모델로 같은 변경을 다시 리뷰한다. 다만 모델이 둘, 셋으로 늘어도 합의가 항상 유효한 원인을 보장하진 않았다. 여러 모델이 반복 분석해 합의한 버그 원인이 실제로는 틀린 경우도 겪었다. 결국 마지막 단계는 사람이 가정과 반례를 명시하고, 실패 시나리오를 재현해 확인하는 절차로 닫는 게 가장 안정적이었다.
