---
title: 실코드베이스에서 AI 코딩 워크플로를 설계할 때 본 기준
description: 온보딩, 커스텀 MCP, 리뷰 자동화 실험, 컨텍스트·토큰 관리까지 실제 적용에서 정리한 운영 기준.
pubDate: 2025-03-06T07:14:47+00:00
source: twitter
tags:
  - ai-coding
  - mcp
  - workflow
  - token-economics
---

AI 코딩을 실서비스 코드베이스에 붙일 때 먼저 보인 건 모델 점수보다 연결 방식이었다. Cursor를 높게 보는 이유도 코드베이스를 읽고 쓰는 프로토콜에서 나온 시너지였고, Claude Code를 처음 썼을 때 온보딩이 매끄러웠던 지점도 같았다. IDE 맥락을 잘 물고, 현재 동작을 비교적 자세히 보여주며 반응이 빠르면 초반 탐색 시간이 줄어든다. 반대로 이 연결이 약하면 모델이 좋아도 왕복이 길어진다.

공식 통합이 비어 있는 영역은 직접 메웠다. Datadog은 공식 MCP가 없어 SDK 타입을 뽑고 리플렉션으로 런타임 명세를 만드는 쪽을 택했다. 비공식 npm 패키지를 바로 신뢰하기 어려웠기 때문이다. 구현 부담은 생기지만, 어떤 스키마를 노출하고 어떻게 바꿀지 통제권을 가져갈 수 있다. 같은 맥락에서 쿼리형 MCP를 메인 세션에 계속 붙이는 방식도 조심했다. 조회성 질의가 많아질수록 메인 컨텍스트가 금방 비대해져, 필요한 요약만 남기는 분리 전략이 더 현실적이었다.

운영 단계에서는 자동화와 비용을 같이 봤다. 로컬 웹훅을 열고 구독 계정이 붙은 환경에서 레포를 클론해 브랜치 리뷰를 돌리는 구성도 실험했는데, 이건 아직 결과를 단정하기 어렵다. 모델 선택도 하나로 고정하지 않았다. 복잡한 개발·디버깅은 상위 모델이 안정적이고, 반복성 높은 일상 작업은 가성비 모델이 유리했다. API 토큰을 상대적으로 부담 없이 쓰는 환경에서는 아쉬운 품질이 일부 상쇄되기도 했다.

다만 “패턴이 정형화된 작업이면 자동으로 잘 된다”는 가정은 여러 번 틀렸다. 스프링에서 인메모리 캐시와 Redis를 함께 쓰는 상황에서 테스트를 만들다 빈 주입 오류를 못 잡은 경험이 있었다. 이때 배운 건 단순했다. 모델 교체보다 먼저 검증 경계를 작게 나누고, 메인 세션의 맥락을 얇게 유지하고, 통합 포인트를 직접 통제하는 편이 실패 복구가 빠르다. 아직 완성된 정답은 없지만, 이 기준은 속도와 신뢰성, 토큰 비용 사이의 타협점을 꾸준히 조정하게 해준다.
